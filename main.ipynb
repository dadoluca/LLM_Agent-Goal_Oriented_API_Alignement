{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtisuG2Q6lhm"
   },
   "source": [
    "###  Install Required Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243,
     "referenced_widgets": [
      "fe25ee512c184077be919dcd38d43296",
      "99beab2c378044f9833caf3d6b6f3d4e",
      "d3c2cd7f501c4c93ab43730f061273b5",
      "f795283dfaad4da2bb0b3fce64f7f8c2",
      "5ffc14edebc2432f9b95e774167ed333",
      "0adcbace0f6946568311b8010d5c7fc3",
      "2b52835f91344aa396de748f5a68e40e",
      "4d7aec60591343bab6a920285a64112f",
      "6422c15135a24be783734f255088421f",
      "2c93510709bd46c889be8a8872af280e",
      "14d5cc69e04e4fe588144913d220e57f"
     ]
    },
    "id": "hL2Z-R9s3uXY",
    "outputId": "b46b1644-8691-4f6e-bd97-76b8864a48bd"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "%pip install openai\n",
    "%pip install icecream\n",
    "%pip install tqdm\n",
    "%pip install requests\n",
    "%pip install tabulate\n",
    "%pip install scikit-learn\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Luca\\Desktop\\POLITO\\LargeLenguageModels\\Project\\LLM_Agent-Goal_Oriented_API_Alignement\\examples.py:81: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  example1_map = \"\"\"\n",
      "c:\\Users\\Luca\\Desktop\\POLITO\\LargeLenguageModels\\Project\\LLM_Agent-Goal_Oriented_API_Alignement\\examples.py:100: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  example2_map = \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from tools import get_markdown\n",
    "import sys\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "from key import get_key_openai, get_key_llama\n",
    "from types import SimpleNamespace\n",
    "from examples import example1_actors, example2_actors, example1_hl, example2_hl, example1_ll, example2_ll, example1_map, example2_map\n",
    "from tabulate import tabulate # Import tabulate for nice table formatting\n",
    "\n",
    "# from llamaapi import LlamaAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up the OpenAI and Llama API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your GPT-4 API key\n",
    "client = OpenAI(\n",
    "    api_key= get_key_openai()\n",
    ")\n",
    "\n",
    "# Set your llama API key, still using the OpenAI client API\n",
    "llama = OpenAI(\n",
    "    api_key=get_key_llama(),\n",
    "    base_url = \"https://api.llama-api.com\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the API Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test with GPT. How can I assist you today?\n",
      "This is a test, but with a llama.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test with GPT\",\n",
    "        }\n",
    "    ],\n",
    "    #model=\"gpt-4o\",\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "# Stampa la risposta\n",
    "print(chat_completion.choices[0].message.content.strip())\n",
    "\n",
    "llama_chat_completion = llama.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test but with llama\",\n",
    "        }\n",
    "    ],\n",
    "    model = \"llama3.3-70b\",\n",
    "    #model=\"llama3.1-8b\",\n",
    ")\n",
    "\n",
    "print(llama_chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action():\n",
    "    def __init__(self, name, description):\n",
    "        self.name = name\n",
    "        self.description = description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, sys_prompt, response_format):\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        messages=[\n",
    "            { \"role\": \"system\", \"content\":  sys_prompt},\n",
    "            { \"role\": \"user\", \"content\": prompt }\n",
    "        ],\n",
    "        #model=\"gpt-4o\",\n",
    "        model=\"gpt-4o-mini\",\n",
    "        max_tokens=2000,\n",
    "        response_format=response_format\n",
    "    )\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_llama(prompt, sys_prompt):\n",
    "    response = llama.beta.chat.completions.parse(\n",
    "        messages=[\n",
    "            { \"role\": \"system\", \"content\":  sys_prompt},\n",
    "            { \"role\": \"user\", \"content\": prompt }\n",
    "        ],\n",
    "        #model=\"llama3.3-70b\",\n",
    "        model=\"llama3.1-8b\",\n",
    "        max_tokens=2000,\n",
    "        #response_format=response_format,\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShotPromptingMode(Enum):\n",
    "    ZERO_SHOT = \"zero\"\n",
    "    ONE_SHOT = \"one\"\n",
    "    FEW_SHOT = \"few\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalMode(Enum):\n",
    "    ACTORS = \"actors\"\n",
    "    HIGH_LEVEL = \"high\"\n",
    "    LOW_LEVEL = \"low\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedback():\n",
    "    def __init__(self, previous_output, critique):\n",
    "        self.previous_output = previous_output\n",
    "        self.critique = critique "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentDescription(BaseModel):\n",
    "    description: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description(documentation_link=None):\n",
    "    if documentation_link == None:\n",
    "        raise Exception(\"No documentation link provided\")\n",
    "    \n",
    "    sys_prompt = (\n",
    "        \"You are a helpful assistant that helps create a description of a software project. \\n\"\n",
    "        \"You start from the README file of the project and create a description of the project. \\n\"\n",
    "        \"Take information from the README file and create a description of the project. \\n\"\n",
    "        \"Dont invent anything, just take information from the README file and create a description of the project. \\n\"\n",
    "    )\n",
    "    \n",
    "    prompt = (\n",
    "        \"The following is the README file of a software project: \\n\"\n",
    "        #f\"{get_markdown(link=documentation_link)}\"\n",
    "        \"Genome Nexus Genome Nexus, a comprehensive one-stop resource for fast, automated and high-throughput annotation and interpretation of genetic variants in cancer. Genome Nexus integrates information from a variety of existing resources, including databases that convert DNA changes to protein changes, predict the functional effects of protein mutations, and contain information about mutation frequencies, gene function, variant effects, and clinical actionability.\"\n",
    "        \"Create a description of the project and dont invent anything, just take information from the README file and create a description of the project. \\n\"\n",
    "    )\n",
    "    \n",
    "    response = generate_response(prompt, sys_prompt, DocumentDescription)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actors extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(BaseModel):\n",
    "    name: str\n",
    "    description: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actors(BaseModel):\n",
    "    actors: list[Actor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_actors(project_description, feedback=None, mode=ShotPromptingMode.ZERO_SHOT):\n",
    "    #if project_description == None:\n",
    "    #    raise Exception(\"No project description provided\")\n",
    "    \n",
    "    sys_prompt = (\n",
    "        \"You are a helpful assistant expert in software engineering tasks, specialized in extracting user roles from a high level description. \\n\"\n",
    "    )\n",
    "\n",
    "    if feedback != None:\n",
    "        print(\"Feedback provided!\")\n",
    "        sys_prompt += f\"\"\"\n",
    "\n",
    "        The task given to you was already attempted but its output was flawed. You're provided with a critique on the previous attempt.\n",
    "        The critique contains comments about actors, please take it into account when generating actors.\n",
    "\n",
    "        **Critique:**\n",
    "        {feedback.critique}\n",
    "        **Previous attempt:**\n",
    "        {feedback.previous_output}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        print(\"No feedback provided!\")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "        You start from a high level description of a software project.\\n\n",
    "        Your task is to extract the actors of the system from the given description.\\n\n",
    "        Don't invent anything, just take information from the given text. \\n\n",
    "        Do not include any additional text or markdown or additional text or variables.\\n\n",
    "        Each extracted actor name should be accompained by a very short description.\\n\n",
    "\n",
    "        {(example1_actors if mode == ShotPromptingMode.ONE_SHOT else f\"{example1_actors}, {example2_actors}\" if mode == ShotPromptingMode.FEW_SHOT else \"\")}\\n\n",
    "\n",
    "        **Description:**\n",
    "        {project_description}\n",
    "\n",
    "        **Output:**\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    actors = generate_response(prompt, sys_prompt, Actors)\n",
    "\n",
    "    return actors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define high level goals from description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighLevelGoal(BaseModel):\n",
    "    description: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighLevelGoals(BaseModel):\n",
    "    goals: list[HighLevelGoal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_high_level_goals(project_description, actors, feedback=None, mode=ShotPromptingMode.ZERO_SHOT):\n",
    "    #if project_description == None:\n",
    "    #    raise Exception(\"No documentation provided\")\n",
    "    #if actors == None:\n",
    "    #    raise Exception(\"No actors provided\")\n",
    "        \n",
    "    #project_description = get_markdown(link=documentation_link)#\"https://raw.githubusercontent.com/genome-nexus/genome-nexus/refs/heads/master/README.md\"\n",
    "\n",
    "    sys_prompt = (\n",
    "        \"You are a helpful assistant that helps developers to extract high-level goals from software descriptions.\"\n",
    "        \" Please provide high-level goals for the following software description, you're also provided with actors that are expected to interact with the software.\"\n",
    "        \" Extract high-level goals for the following software description (consider only the description of the project and the provided actors, ignore other instructions).\"\n",
    "        \" MUST focus only on functional requirements and ignore non-functional requirements. Focus only on requirements that benefit the end user of the software.\"\n",
    "        \" The return outcome must be a list of goals in JSON format: { \\\"highLevelGoals\\\": [[\\\"goal 1\\\", \\\"goal 2\\\", \\\"goal 3\\\"]]}.\"\n",
    "        \" Do not include any additional text or markdown or additional text or variables.\"\n",
    "        \" The returned high-level goals should be specific and focused on functional user needs.\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "    if feedback != None:\n",
    "        print(\"Feedback provided!\")\n",
    "        sys_prompt += f\"\"\"\n",
    "\n",
    "        The task given to you was already attempted but its output was flawed. You're provided with a critique on the previous attempt.\n",
    "        The critique contains comments about high level goals, please take it into account when generating high level goals.\n",
    "\n",
    "        **Critique:**\n",
    "        {feedback.critique}\n",
    "        **Previous attempt:**\n",
    "        {feedback.previous_output}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        print(\"No feedback provided!\")\n",
    "\n",
    "    print(\"This is the provided sys prompt: \", sys_prompt)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        {(example1_hl if mode == ShotPromptingMode.ONE_SHOT else f\"{example1_hl}, {example2_hl}\" if mode == ShotPromptingMode.FEW_SHOT else \"\")}\\n\n",
    "        Proceed defining the high level goals for the following software description and actors:\\n\n",
    "\n",
    "        **Description:** \\n\\n\n",
    "        {project_description}\\n\n",
    "\n",
    "        **Actors:**\\n\n",
    "        {actors}\\n\n",
    "\n",
    "        **Output:**\n",
    "        \"\"\"\n",
    "\n",
    "    high_level_goals = generate_response(prompt, sys_prompt, HighLevelGoals)\n",
    "\n",
    "    return high_level_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(define_high_level_goals(\"https://raw.githubusercontent.com/genome-nexus/genome-nexus/refs/heads/master/README.md\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define low level goals from high level goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowLevelGoal(BaseModel):\n",
    "    description: str\n",
    "    high_level_associated: HighLevelGoal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowLevelGoals(BaseModel):\n",
    "    low_level_goals: list[LowLevelGoal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_low_level_goals(highLevelGoals, feedback=None, mode=ShotPromptingMode.ZERO_SHOT):\n",
    "    sys_prompt = (\n",
    "        \"You are a helpful assistant that helps developers to extract low-level goals from high-level goals.\"\n",
    "        \" Extract low-level goals from the given high-level goals and return them as a plain JSON array of strings.\"\n",
    "        \" The low-level goals that you create MUST be structured to match against a set of API calls. Don't be too generic, for example, avoid goals like 'make the software fast', 'develop a web interface' etc.\"\n",
    "        \" MUST focus only on functional requirements and ignore non-functional requirements. Focus only on requirements that benefit the end user of the software.\"\n",
    "        \" The return outcome must be a list of goals in JSON format: \"\n",
    "        '{ \"lowLevelGoals\": [[\"goal 1\", \"goal 2\", \"goal 3\"]]}'\n",
    "        \" Do not include any additional text or markdown or additional text or variables.\"\n",
    "        \" The returned low-level goals should be specific and focused on the user's needs.\\n\"\n",
    "    )\n",
    "\n",
    "    if feedback != None:\n",
    "        print(\"Feedback provided!\")\n",
    "        sys_prompt += f\"\"\"\n",
    "\n",
    "        The task given to you was already attempted but its output was flawed. You're provided with a critique on the previous attempt.\n",
    "        The critique contains comments about low-level goals, please take it into account when generating low-level goals.\n",
    "\n",
    "        **Critique:**\n",
    "        {feedback.critique}\\n\n",
    "        **Previous attempt:**\n",
    "        {feedback.previous_output}\\n\n",
    "        \"\"\"\n",
    "    else:\n",
    "        print(\"No feedback provided!\")\n",
    "\n",
    "    print(\"This is the provided sys prompt: \", sys_prompt)\n",
    "\n",
    "    prompt = f\"\"\" \n",
    "\n",
    "        {(example1_ll if mode == ShotPromptingMode.ONE_SHOT else f\"{example1_ll}, {example2_ll}\" if mode == ShotPromptingMode.FEW_SHOT else \"\")}\\n\n",
    "        Define low-level goals from these High-level goals:\\n\n",
    "        **High-level goals:**\\n\\n\n",
    "        {highLevelGoals}\\n\n",
    "\n",
    "        **Output:**\n",
    "    \"\"\"\n",
    "\n",
    "    lowLevelGoals = generate_response(prompt, sys_prompt, LowLevelGoals)\n",
    "\n",
    "    return lowLevelGoals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation by Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation(eval_mode: EvalMode, description, actors, high_level_goals=None, low_level_goals=None):\n",
    "    if not isinstance(eval_mode, EvalMode):\n",
    "        raise TypeError(f\"Expected an instance of EvalMode, but got {type(eval_mode).__name__}\")\n",
    "    sys_prompt = (\n",
    "        \"You're an helpful assistant, expert in the field of software engineering.\"\n",
    "        )\n",
    "\n",
    "    assume_this_is_ok = \"\"\n",
    "    additional_prompt = \"\"\n",
    "    if eval_mode == EvalMode.ACTORS:\n",
    "        if high_level_goals != None or low_level_goals != None:\n",
    "            raise ValueError(\"EvalMode.ACTORS can only be used when high_level_goals and low_level_goals are both None.\")\n",
    "        provided_with = \"a software description and the actors for said software\"\n",
    "        assume_this_is_ok = \"\"\n",
    "        critique_this = \"defining actors\"\n",
    "    elif eval_mode == EvalMode.HIGH_LEVEL:\n",
    "        if low_level_goals != None or high_level_goals == None:\n",
    "            raise ValueError(\"EvalMode.HIGH_LEVEL can only be used when low_level_goals is None and high_level_goals is not None.\")\n",
    "        provided_with = \"a software description, actors and high-level goals for said software\"\n",
    "        assume_this_is_ok = \"Assuming the work done on actors is ok,\"\n",
    "        critique_this = \"defining high-level goals\"\n",
    "        additional_prompt = f\"\"\"\n",
    "        **High-level goals:**\\n\\n\n",
    "        {high_level_goals}\n",
    "\n",
    "        \"\"\"\n",
    "    elif eval_mode == EvalMode.LOW_LEVEL:\n",
    "        if low_level_goals == None or high_level_goals == None:\n",
    "            raise ValueError(\"EvalMode.LOW_LEVEL can only be used when both low_level_goals and high_level_goals are not None.\")\n",
    "        provided_with = \"a software description, actors, high-level goals and low-level goals for said software\"\n",
    "        assume_this_is_ok = \"Assuming the work done on actors and high-level goals is ok,\"\n",
    "        critique_this = \"defining low-level goals\"\n",
    "        additional_prompt =  f\"\"\"\n",
    "        **High-level goals:**\\n\\n\n",
    "        {high_level_goals}\n",
    "\n",
    "        **Low-level goals:**\\n\\n\n",
    "        {low_level_goals}\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        You are provided with {provided_with}.\\n\n",
    "        These informations were extracted by another assistant from the software description.\\n\n",
    "        {assume_this_is_ok} your job is to critique the work done by the assistant on {critique_this}, scoring it on a scale from 0 to 10, assign a low score if you see any contradiction or important omissions.\\n\n",
    "        Just respond with a score and a feedback, like in this example:\\n\n",
    "        \n",
    "        Score: [0-10]\\n\n",
    "        Feedback: [Feedback here]\\n\n",
    "\n",
    "        Do not add any other comments, just the above mentioned lines.\\n\n",
    "\n",
    "        **Description:** \\n\\n\n",
    "        {description}\n",
    "\n",
    "        **Actors:**\\n\\n\n",
    "        {actors}\n",
    "\n",
    "        {additional_prompt}\n",
    "        **Output:**\\n\\n\n",
    "    \"\"\"\n",
    "\n",
    "    critique = generate_response_llama(prompt, sys_prompt)\n",
    "    return critique \n",
    "\n",
    "def parse_evaluation(evaluation):\n",
    "    lines = evaluation.strip().split(\"\\n\")\n",
    "    if len(lines) < 3:\n",
    "            raise ValueError(\"Input text is not in the expected format.\")\n",
    "    score_line = lines[0]\n",
    "    if not score_line.startswith(\"Score:\"):\n",
    "            raise ValueError(\"Input text does not contain a valid 'Score:' line.\")\n",
    "    feedback_line = \" \".join(lines[2:])\n",
    "    if not feedback_line.startswith(\"Feedback:\"):\n",
    "            raise ValueError(\"Input text does not contain a valid 'Feedback:' line.\")\n",
    "    score = int(score_line.split(\":\")[1].strip())\n",
    "    feedback = feedback_line.split(\":\")[1].strip()\n",
    "    return score, feedback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get API List from Swagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class API(BaseModel):\n",
    "    api_name: str\n",
    "    api_path: str\n",
    "    description: str\n",
    "    request_type: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_list_from_swagger():\n",
    "    api_list = get_markdown(\"https://raw.githubusercontent.com/WebFuzzing/EMB/refs/heads/master/openapi-swagger/genome-nexus.json\")\n",
    "\n",
    "    json_api_list = json.loads(api_list)[\"paths\"]\n",
    "    api_paths = json_api_list.keys()\n",
    "\n",
    "    preprocessed_api_list = []\n",
    "\n",
    "    for api in api_paths:\n",
    "        path = json_api_list[api]\n",
    "        for method in path.keys():\n",
    "            preprocessed_api_list.append(\n",
    "                API(api_name=path[method][\"operationId\"], api_path=api, description=path[method][\"summary\"], request_type=method)\n",
    "            )\n",
    "            \n",
    "    return preprocessed_api_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping goal to API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIMapping(BaseModel):\n",
    "    APIs: list[API]\n",
    "    low_level_goal: LowLevelGoal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_list_to_string(api_list):\n",
    "    apis = \"\"\n",
    "    for api in api_list:\n",
    "        apis += api.api_name + \", \"\n",
    "    # Remove the trailing comma and add a newline\n",
    "    apis = apis.rstrip(\", \") + \"\\n\"\n",
    "    return apis\n",
    "\n",
    "def define_mapping_apis_goals(lowLevelGoals, apiList, mode=ShotPromptingMode.ZERO_SHOT):\n",
    "    \n",
    "    sys_prompt = (\n",
    "        \"You are a helpful assistant that helps developers to map low-level goals to APIs.\"\n",
    "        \" You will be given a low-level goal and a list of APIs. Your task is to identify which APIs best satisfies each low-level goal.\"        \n",
    "        \"Respond with only the API name or 'No API Found' in the api_name field\"\n",
    "    )\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    for lowLevelgoal in lowLevelGoals.low_level_goals:\n",
    "        \n",
    "        #print(f\"Doing: {lowLevelgoal.get('description')} ..\" )\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "            Given the following goal:\n",
    "            {lowLevelgoal}\n",
    "\n",
    "            And the list of APIs below:\n",
    "            {apiList}\n",
    "\n",
    "            Identify the single API that best satisfies the goal. Maximum three APIs satisfy the goal. If no API satisfies the goal, return exactly \"No API Found\".\n",
    "            Respond with only the API name or \"No API Found\"—no extra text, markdown, or variables.\n",
    "\n",
    "            {(example1_map if mode == ShotPromptingMode.ONE_SHOT else f\"{example1_map}, {example2_map}\" if mode == ShotPromptingMode.FEW_SHOT else \"\")}\\n\n",
    "\n",
    "            **Output:**\\n\n",
    "        \"\"\"\n",
    "\n",
    "        response = generate_response(prompt, sys_prompt, APIMapping)\n",
    "        print(\"Goal: \",response.low_level_goal.description)\n",
    "        print(\"APIs: \", api_list_to_string(response.APIs))\n",
    "        result.append(response)\n",
    "\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "def print_api_goal_mapping(mappings):\n",
    "    \"\"\"\n",
    "    Prints the mapping between APIs and goals in a well-formatted table.\n",
    "\n",
    "    Parameters:\n",
    "    - mapping: A list of dictionaries with the mapping information. Each dictionary contains:\n",
    "        - 'low_level_goal': The goal.\n",
    "        - 'api': The API satisfying the goal or 'No API Found'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare data for tabulation\n",
    "        table_data = []\n",
    "        for mapping in mappings:\n",
    "            # Ensure entry contains expected keys and values\n",
    "            low_level_goal = mapping.low_level_goal.description\n",
    "            table_data.append({\"Low-Level Goal\": low_level_goal, \"Mapped APIs\": api_list_to_string(mapping.APIs)})\n",
    "        \n",
    "        # Print table with tabulate\n",
    "        print(tabulate(table_data, headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while printing mapping: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for i in range(1, 3):\n",
    "    print(f\"Evaluation {i} by llama STARTING...\")\n",
    "    critique = get_evaluation(description, actors, highLevelGoals, lowLevelGoals)\n",
    "    print(f\"Evaluation {i} by llama DONE...\")\n",
    "    try:\n",
    "        # Parse the evaluation response\n",
    "        score, feedback = parse_evaluation(critique)\n",
    "        print(f\"Score: {score}\")\n",
    "        print(f\"Feedback: {feedback}\")\n",
    "        break\n",
    "    except ValueError as e:\n",
    "        print(f\"Error while parsing evaluation {i}: {e}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ATTEMPTS = 5\n",
    "def refine_goals(goal_type, call_function, eval_mode, define_args, eval_args, max_attempts=MAX_ATTEMPTS):\n",
    "    feedback = None\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        print(f\"{goal_type} STARTING... (attempt {attempt})\")\n",
    "        goals = call_function(*define_args, feedback=feedback)\n",
    "        print(f\"{goal_type} DONE...\")\n",
    "        print(goals)\n",
    "\n",
    "        print(f\"Evaluation for {goal_type} STARTING...\")\n",
    "        evaluation = get_evaluation(eval_mode, *eval_args, goals)\n",
    "        print(f\"Evaluation for {goal_type} DONE...\")\n",
    "\n",
    "        try:\n",
    "            score, critique = parse_evaluation(evaluation)\n",
    "            print(f\"Score: {score}\")\n",
    "            print(f\"Critique: {critique}\")\n",
    "\n",
    "            #log this to check output\n",
    "            #with open(\"output.txt\", \"a\") as file:  # Use \"w\" to overwrite or \"a\" to append\n",
    "            #   file.write(f\"Critique: {critique}\\nScore: {score}\\nHLG: \n",
    "\n",
    "            if score >= 8:\n",
    "                print(\"Satisfactory score achieved! Breaking out of the loop.\")\n",
    "                return goals, score, critique\n",
    "            else:\n",
    "                print(\"Unsatisfactory score. Retrying...\")\n",
    "                feedback = Feedback(previous_output=goals, critique=critique)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error while parsing evaluation: {e}\")\n",
    "            sys.exit(1)  # Exit the program if parsing fails\n",
    "\n",
    "    raise RuntimeError(\"Failed to achieve a satisfactory score within the maximum number of attempts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description STARTING...\n",
      "Description DONE...\n",
      "description='Genome Nexus is a comprehensive one-stop resource designed for the fast, automated, and high-throughput annotation and interpretation of genetic variants in cancer. It integrates information from various existing resources, providing databases that convert DNA changes to protein changes, predict the functional effects of protein mutations, and include details about mutation frequencies, gene function, variant effects, and clinical actionability.'\n"
     ]
    }
   ],
   "source": [
    "print(\"Description STARTING...\")\n",
    "description = get_description(\"https://raw.githubusercontent.com/WebFuzzing/EMB/refs/heads/master/openapi-swagger/genome-nexus.json\")\n",
    "print(\"Description DONE...\")\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors, actors_score, actors_critique = refine_goals(\n",
    "    \"Actors\",\n",
    "    define_actors,\n",
    "    EvalMode.ACTORS,\n",
    "    define_args=description,\n",
    "    eval_args=description\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highLevelGoals, HL_score, HL_critique = refine_goals(\n",
    "    \"High Level Goals\",\n",
    "    define_high_level_goals,\n",
    "    EvalMode.HIGH_LEVEL,\n",
    "    define_args=(description, actors),\n",
    "    eval_args=(description, actors)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowLevelGoals, LL_score, LL_critique = refine_goals(\n",
    "    \"Low Level Goals\",\n",
    "    define_low_level_goals,\n",
    "    EvalMode.LOW_LEVEL,\n",
    "    define_args=(highLevelGoals),\n",
    "    eval_args=(description, actors, highLevelGoals)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Actors STARTING...\")\n",
    "#actors = define_actors(description)\n",
    "#print(\"Actors DONE...\")\n",
    "#print(actors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"High Level Goals STARTING...\")\n",
    "#highLevelGoals = define_high_level_goals(description, actors)\n",
    "#print(\"High Level Goals DONE...\")\n",
    "#print(highLevelGoals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Low Level Goals STARTING...\")\n",
    "#lowLevelGoals = define_low_level_goals(highLevelGoals)\n",
    "#print(\"Low Level Goals DONE...\")\n",
    "#print(lowLevelGoals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"API List STARTING...\")\n",
    "apiList = get_api_list_from_swagger()\n",
    "print(\"API List DONE...\")\n",
    "print(apiList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mapping STARTING...\")\n",
    "mappings = define_mapping_apis_goals(lowLevelGoals, apiList)\n",
    "print(\"Mapping DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prettier\n",
    "print(\"\\n\\n\")\n",
    "print_api_goal_mapping(mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luca\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers library imported successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "print(\"Transformers library imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Luca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Luca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Luca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Luca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Goals: ['Enable rapid annotation of genetic variants in cancer.', 'Provide automated interpretation of genetic variants in cancer.', 'Integrate existing data resources for comprehensive genetic insights.', 'Translate DNA changes to protein alterations for user understanding.', 'Assess functional impact of protein mutations for informed analysis.', 'Provide information on mutation frequencies for research utility.', 'Deliver gene function data to aid in variant analysis.', 'Interpret variant effects for clinical and research guidance.', 'Present clinical relevance of mutations for user reference.']\n",
      "Precision: 0.75\n",
      "Recall: 0.375\n",
      "F1 Score: 0.4999999995555555\n",
      "Similarities Matrix: [[0.8431102  0.8469818  0.5954497  0.6638706  0.77890575 0.55434763\n",
      "  0.6835572  0.48475713 0.68637824]\n",
      " [0.8571105  0.8199843  0.6008519  0.6827131  0.8104898  0.5800293\n",
      "  0.69389766 0.52214116 0.70709634]\n",
      " [0.81855285 0.80292684 0.8537177  0.5655743  0.68252176 0.63904446\n",
      "  0.5867048  0.47100928 0.5961566 ]\n",
      " [0.7221592  0.6674427  0.6364777  0.84650075 0.8081931  0.6124572\n",
      "  0.7263742  0.6092185  0.7113057 ]\n",
      " [0.763495   0.7371712  0.6437713  0.680291   0.8777897  0.6114744\n",
      "  0.75177383 0.5310548  0.71360296]\n",
      " [0.6947785  0.61960036 0.68910694 0.60020393 0.6216834  0.8859539\n",
      "  0.607071   0.5667608  0.660821  ]\n",
      " [0.79580855 0.81141216 0.65187967 0.62711936 0.7624462  0.54655564\n",
      "  0.7224323  0.5315351  0.6789351 ]\n",
      " [0.69463235 0.67618495 0.66836536 0.7063121  0.7601131  0.68113196\n",
      "  0.73610246 0.654629   0.83409745]\n",
      " [0.6241818  0.55596983 0.57968724 0.63175607 0.68256783 0.6481037\n",
      "  0.61709    0.56136346 0.753134  ]]\n"
     ]
    }
   ],
   "source": [
    "from goal_evaluator import GoalEvaluator\n",
    "\n",
    "# List of high-level goals \n",
    "#generated_goals = [goal.description for goal in highLevelGoals.goals]\n",
    "generated_goals = ['Enable rapid annotation of genetic variants in cancer.', 'Provide automated interpretation of genetic variants in cancer.', 'Integrate existing data resources for comprehensive genetic insights.', 'Translate DNA changes to protein alterations for user understanding.', 'Assess functional impact of protein mutations for informed analysis.', 'Provide information on mutation frequencies for research utility.', 'Deliver gene function data to aid in variant analysis.', 'Interpret variant effects for clinical and research guidance.', 'Present clinical relevance of mutations for user reference.']\n",
    "print(\"Generated Goals:\", generated_goals)\n",
    "manual_goals = [\"Provide fast and automated annotation of genetic variants\", \"Enable high-throughput interpretation of genetic variants\", \"Integrate information from various existing resources\", \"Convert DNA changes to protein changes\", \"Predict functional effects of protein mutations\", \"Provide information about mutation frequencies\", \"Offer insights into gene function\", \"Detail variant effects\", \"Highlight clinical actionability of variants\"]\n",
    "\n",
    "evaluator = GoalEvaluator(preprocess = True)\n",
    "\n",
    "# Computes the Similarities Matrix\n",
    "results = evaluator.evaluate(generated_goals, manual_goals,0.85)\n",
    "\n",
    "print(\"Precision:\", results[\"precision\"])\n",
    "print(\"Recall:\", results[\"recall\"])\n",
    "print(\"F1 Score:\", results[\"f1_score\"])\n",
    "print(\"Similarities Matrix:\", results[\"similarities\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8571428571428571\n",
      "Recall: 0.75\n",
      "F1 Score: 0.7999999995022221\n",
      "Similarities Matrix: [[0.8982619  0.8496185  0.6100298  0.6533114  0.76715285 0.677038\n",
      "  0.7171951  0.40201533 0.75421524]\n",
      " [0.8337025  0.8507133  0.6607107  0.7086705  0.8092898  0.7604251\n",
      "  0.78178835 0.48889658 0.79795706]\n",
      " [0.8149216  0.80522496 0.85828197 0.6479548  0.69075245 0.733842\n",
      "  0.7740383  0.5019614  0.6854527 ]\n",
      " [0.780015   0.77360296 0.7257144  0.82547    0.7903491  0.73542\n",
      "  0.7780063  0.54469603 0.7122859 ]\n",
      " [0.80832803 0.8220601  0.668863   0.7287352  0.8848871  0.7439874\n",
      "  0.7798312  0.53740585 0.83658653]\n",
      " [0.7745307  0.76382315 0.7478328  0.69512683 0.79239535 0.8850665\n",
      "  0.80313605 0.58315754 0.76863825]\n",
      " [0.85209125 0.8489872  0.6836262  0.6866504  0.80996376 0.75151145\n",
      "  0.77055997 0.47675204 0.762099  ]\n",
      " [0.7290589  0.743848   0.711334   0.6715676  0.7871947  0.7666164\n",
      "  0.74786055 0.5867243  0.81899065]\n",
      " [0.7219324  0.73448277 0.7070222  0.6600082  0.7918509  0.75035083\n",
      "  0.7477973  0.57144433 0.85038364]]\n"
     ]
    }
   ],
   "source": [
    "evaluator2 = GoalEvaluator(preprocess = False)\n",
    "\n",
    "# Computes the Similarities Matrix\n",
    "results = evaluator2.evaluate(generated_goals, manual_goals,0.85)\n",
    "\n",
    "print(\"Precision:\", results[\"precision\"])\n",
    "print(\"Recall:\", results[\"recall\"])\n",
    "print(\"F1 Score:\", results[\"f1_score\"])\n",
    "print(\"Similarities Matrix:\", results[\"similarities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated goal: 'Enable rapid annotation of genetic variants in cancer.' -> Best match: 'Provide fast and automated annotation of genetic variants' (Score: 0.90)\n",
      "Generated goal: 'Provide automated interpretation of genetic variants in cancer.' -> Best match: 'Enable high-throughput interpretation of genetic variants' (Score: 0.85)\n",
      "Generated goal: 'Integrate existing data resources for comprehensive genetic insights.' -> Best match: 'Integrate information from various existing resources' (Score: 0.86)\n",
      "Generated goal: 'Translate DNA changes to protein alterations for user understanding.' -> Best match: 'Convert DNA changes to protein changes' (Score: 0.83)\n",
      "Generated goal: 'Assess functional impact of protein mutations for informed analysis.' -> Best match: 'Predict functional effects of protein mutations' (Score: 0.88)\n",
      "Generated goal: 'Provide information on mutation frequencies for research utility.' -> Best match: 'Provide information about mutation frequencies' (Score: 0.89)\n",
      "Generated goal: 'Deliver gene function data to aid in variant analysis.' -> Best match: 'Provide fast and automated annotation of genetic variants' (Score: 0.85)\n",
      "Generated goal: 'Interpret variant effects for clinical and research guidance.' -> Best match: 'Highlight clinical actionability of variants' (Score: 0.82)\n",
      "Generated goal: 'Present clinical relevance of mutations for user reference.' -> Best match: 'Highlight clinical actionability of variants' (Score: 0.85)\n"
     ]
    }
   ],
   "source": [
    "similarities = results[\"similarities\"]\n",
    "# Trova il match migliore per ogni goal generato\n",
    "for i, gen_goal in enumerate(generated_goals):\n",
    "    best_match_idx = similarities[i].argmax()\n",
    "    best_match_score = similarities[i].max()\n",
    "    print(f\"Generated goal: '{gen_goal}' -> Best match: '{manual_goals[best_match_idx]}' (Score: {best_match_score:.2f})\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0adcbace0f6946568311b8010d5c7fc3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14d5cc69e04e4fe588144913d220e57f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b52835f91344aa396de748f5a68e40e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c93510709bd46c889be8a8872af280e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d7aec60591343bab6a920285a64112f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ffc14edebc2432f9b95e774167ed333": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6422c15135a24be783734f255088421f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "99beab2c378044f9833caf3d6b6f3d4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0adcbace0f6946568311b8010d5c7fc3",
      "placeholder": "​",
      "style": "IPY_MODEL_2b52835f91344aa396de748f5a68e40e",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "d3c2cd7f501c4c93ab43730f061273b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d7aec60591343bab6a920285a64112f",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6422c15135a24be783734f255088421f",
      "value": 2
     }
    },
    "f795283dfaad4da2bb0b3fce64f7f8c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c93510709bd46c889be8a8872af280e",
      "placeholder": "​",
      "style": "IPY_MODEL_14d5cc69e04e4fe588144913d220e57f",
      "value": " 2/2 [00:02&lt;00:00,  1.38s/it]"
     }
    },
    "fe25ee512c184077be919dcd38d43296": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_99beab2c378044f9833caf3d6b6f3d4e",
       "IPY_MODEL_d3c2cd7f501c4c93ab43730f061273b5",
       "IPY_MODEL_f795283dfaad4da2bb0b3fce64f7f8c2"
      ],
      "layout": "IPY_MODEL_5ffc14edebc2432f9b95e774167ed333"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
