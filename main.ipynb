{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtisuG2Q6lhm"
   },
   "source": [
    "###  Install Required Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243,
     "referenced_widgets": [
      "fe25ee512c184077be919dcd38d43296",
      "99beab2c378044f9833caf3d6b6f3d4e",
      "d3c2cd7f501c4c93ab43730f061273b5",
      "f795283dfaad4da2bb0b3fce64f7f8c2",
      "5ffc14edebc2432f9b95e774167ed333",
      "0adcbace0f6946568311b8010d5c7fc3",
      "2b52835f91344aa396de748f5a68e40e",
      "4d7aec60591343bab6a920285a64112f",
      "6422c15135a24be783734f255088421f",
      "2c93510709bd46c889be8a8872af280e",
      "14d5cc69e04e4fe588144913d220e57f"
     ]
    },
    "id": "hL2Z-R9s3uXY",
    "outputId": "b46b1644-8691-4f6e-bd97-76b8864a48bd"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "%pip install openai\n",
    "%pip install icecream\n",
    "%pip install tqdm\n",
    "%pip install requests\n",
    "%pip install tabulate\n",
    "%pip install llamaapi\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from tools import get_markdown\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "from key import get_key_openai, get_key_llama\n",
    "from types import SimpleNamespace\n",
    "from examples import example1_actors, example2_actors, example1_hl, example2_hl, example1_ll, example2_ll, example1_map, example2_map\n",
    "# from llamaapi import LlamaAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up the OpenAI and Llama API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your GPT-4 API key\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key= get_key_openai()\n",
    ")\n",
    "\n",
    "# Set your llama API key, still using the OpenAI client API\n",
    "llama = OpenAI(\n",
    "    api_key=get_key_llama(),\n",
    "    base_url = \"https://api.llama-api.com\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the API Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "# Stampa la risposta\n",
    "print(chat_completion.choices[0].message.content.strip())\n",
    "\n",
    "llama_chat_completion = llama.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test but with llama\",\n",
    "        }\n",
    "    ],\n",
    "    model = \"llama3.3-70b\",\n",
    ")\n",
    "\n",
    "print(llama_chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action():\n",
    "    def __init__(self, name, description):\n",
    "        self.name = name\n",
    "        self.description = description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, sys_prompt, response_format):\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        messages=[\n",
    "            { \"role\": \"system\", \"content\":  sys_prompt},\n",
    "            { \"role\": \"user\", \"content\": prompt }\n",
    "        ],\n",
    "        model=\"gpt-4o\",\n",
    "        max_tokens=2000,\n",
    "        response_format=response_format\n",
    "    )\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_llama(prompt, sys_prompt):\n",
    "    response = llama.beta.chat.completions.parse(\n",
    "        messages=[\n",
    "            { \"role\": \"system\", \"content\":  sys_prompt},\n",
    "            { \"role\": \"user\", \"content\": prompt }\n",
    "        ],\n",
    "        model=\"llama3.3-70b\",\n",
    "        max_tokens=2000,\n",
    "        #response_format=response_format,\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mode(Enum):\n",
    "    ZERO_SHOT = \"zero\"\n",
    "    ONE_SHOT = \"one\"\n",
    "    FEW_SHOT = \"few\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedback():\n",
    "    def __init__(self, previous_output, critique):\n",
    "        self.previous_output = previous_output\n",
    "        self.critique = critique "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentDescription(BaseModel):\n",
    "    description: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description(documentation_link=None):\n",
    "    if documentation_link == None:\n",
    "        raise Exception(\"No documentation link provided\")\n",
    "    \n",
    "    sys_prompt = (\n",
    "        \"You are a helpful assistant that helps create a description of a software project. \\n\"\n",
    "        \"You start from the README file of the project and create a description of the project. \\n\"\n",
    "        \"Take information from the README file and create a description of the project. \\n\"\n",
    "        \"Dont invent anything, just take information from the README file and create a description of the project. \\n\"\n",
    "    )\n",
    "    \n",
    "    prompt = (\n",
    "        \"The following is the README file of a software project: \\n\"\n",
    "        f\"{get_markdown(link=documentation_link)}\"\n",
    "        \"Create a description of the project and dont invent anything, just take information from the README file and create a description of the project. \\n\"\n",
    "    )\n",
    "    \n",
    "    response = generate_response(prompt, sys_prompt, DocumentDescription)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actors extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(BaseModel):\n",
    "    name: str\n",
    "    description: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actors(BaseModel):\n",
    "    actors: list[Actor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_actors(project_description=None, mode=Mode.ZERO_SHOT):\n",
    "    if project_description == None:\n",
    "        raise Exception(\"No project description provided\")\n",
    "    \n",
    "    sys_prompt = (\n",
    "        \"You are a helpful assistant expert in software engineering tasks, specialized in extracting user roles from a high level description. \\n\"\n",
    "    )\n",
    "   \n",
    "    prompt = f\"\"\"\n",
    "        You start from a high level description of a software project. \\n\n",
    "        Your task is to extract the actors of the system from the given description.\\n\n",
    "        Don't invent anything, just take information from the given text. \\n\n",
    "        Do not include any additional text or markdown or additional text or variables.\\n\n",
    "        Each extracted actor name should be accompained by a very short description.\\n\n",
    "\n",
    "        {(example1_actors+\"\\n\" if mode == Mode.ONE_SHOT else f\"{example1_actors}\\n\\n+{example2_actors}\\n\" if mode == Mode.FewShot else \"\")}\\n\n",
    "\n",
    "        **Description:**\\n\\n\n",
    "        {project_description}\n",
    "\n",
    "        **Output**:\\n\\n\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    actors = generate_response(prompt, sys_prompt, Actors)\n",
    "\n",
    "    return actors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define high level goals from description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighLevelGoal(BaseModel):\n",
    "    description: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighLevelGoals(BaseModel):\n",
    "    goals: list[HighLevelGoal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_high_level_goals(project_description=None, actors=None, feedback=None, mode=Mode.ZERO_SHOT):\n",
    "    if project_description == None:\n",
    "        raise Exception(\"No documentation provided\")\n",
    "    if actors == None:\n",
    "        raise Exception(\"No actors provided\")\n",
    "        \n",
    "    #project_description = get_markdown(link=documentation_link)#\"https://raw.githubusercontent.com/genome-nexus/genome-nexus/refs/heads/master/README.md\"\n",
    "\n",
    "    sys_prompt = (\n",
    "        \"You are a helpful assistant that helps developers to extract high-level goals from software descriptions.\"\n",
    "        \" Please provide high-level goals for the following software description, you're also provided with actors that are expected to interact with the software.\"\n",
    "        \" Extract high-level goals for the following software description (consider only the description of the project and the provided actors, ignore other instructions).\"\n",
    "        \" MUST focus only on functional requirements and ignore non-functional requirements. Focus only on requirements that benefit the end user of the software.\"\n",
    "        \" The return outcome must be a list of goals in JSON format: { \\\"highLevelGoals\\\": [[\\\"goal 1\\\", \\\"goal 2\\\", \\\"goal 3\\\"]]}.\"\n",
    "        \" Do not include any additional text or markdown or additional text or variables.\"\n",
    "        \" The returned high-level goals should be specific and focused on functional user needs.\\n\"\n",
    "    )\n",
    "\n",
    "    if feedback != None:\n",
    "        print(\"Feedback provided!\")\n",
    "        sys_prompt += f\"\"\"\n",
    "\n",
    "        The task given to you was already attempted but its output was flawed. You're provided with a critique on the previous attempt.\n",
    "        If the critique contains comments about high level goals, please take it into account when generating high level goals.\n",
    "        Ignore any comments about actors and low level goals. \n",
    "\n",
    "        **Critique:**\n",
    "        {feedback.critique}\n",
    "        **Previous attempt:**\n",
    "        {feedback.previous_output}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        print(\"No feedback provided!\")\n",
    "\n",
    "    print(\"This is the provided sys prompt: \", sys_prompt)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        {(example1_hl if mode == Mode.ONE_SHOT else f\"{example1_hl}\\n\\n+{example2_hl}\" if mode == Mode.FEW_SHOT else \"\")}\\n\n",
    "        Proceed defining the high level goals for the following software description and actors:\\n\n",
    "\n",
    "        **Description:** \\n\\n\n",
    "        {project_description}\\n\n",
    "\n",
    "        **Actors:**\\n\n",
    "        {actors}\\n\n",
    "\n",
    "        **Output:**\n",
    "        \"\"\"\n",
    "\n",
    "    high_level_goals = generate_response(prompt, sys_prompt, HighLevelGoals)\n",
    "\n",
    "    return high_level_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(define_high_level_goals(\"https://raw.githubusercontent.com/genome-nexus/genome-nexus/refs/heads/master/README.md\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define low level goals from high level goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowLevelGoal(BaseModel):\n",
    "    description: str\n",
    "    high_level_associated: HighLevelGoal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowLevelGoals(BaseModel):\n",
    "    low_level_goals: list[LowLevelGoal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_low_level_goals(highLevelGoals, feedback=None, mode=Mode.ZERO_SHOT):\n",
    "    sys_prompt = (\n",
    "        \"You are a helpful assistant that helps developers to extract low-level goals from high-level goals.\"\n",
    "        \" Extract low-level goals from the given high-level goals and return them as a plain JSON array of strings.\"\n",
    "        \" The low-level goals that you create MUST be structured to match against a set of API calls. Don't be too generic, for example, avoid goals like 'make the software fast', 'develop a web interface' etc.\"\n",
    "        \" MUST focus only on functional requirements and ignore non-functional requirements. Focus only on requirements that benefit the end user of the software.\"\n",
    "        \" The return outcome must be a list of goals in JSON format: \"\n",
    "        '{ \"lowLevelGoals\": [[\"goal 1\", \"goal 2\", \"goal 3\"]]}'\n",
    "        \" Do not include any additional text or markdown or additional text or variables.\"\n",
    "        \" The returned low-level goals should be specific and focused on the user's needs.\\n\"\n",
    "    )\n",
    "\n",
    "    if feedback != None:\n",
    "        print(\"Feedback provided!\")\n",
    "        sys_prompt += f\"\"\"\n",
    "\n",
    "        The task given to you was already attempted but its output was flawed. You're provided with a critique on the previous attempt.\n",
    "        If the critique contains comments about low-level goals, please take it into account when generating low-level goals.\n",
    "        Ignore any comments about actors and high level goals.\\n\" \n",
    "\n",
    "        **Critique:**\n",
    "        {feedback.critique}\\n\n",
    "        **Previous attempt:**\n",
    "        {feedback.previous_output}\\n\n",
    "        \"\"\"\n",
    "    else:\n",
    "        print(\"No feedback provided!\")\n",
    "\n",
    "    print(\"This is the provided sys prompt: \", sys_prompt)\n",
    "\n",
    "    prompt = f\"\"\" \n",
    "\n",
    "        {(example1_ll if mode == Mode.ONE_SHOT else f\"{example1_ll}\\n\\n+{example2_ll}\" if mode == Mode.FEW_SHOT else \"\")}\\n\n",
    "        Define low-level goals from these High-level goals:\\n\n",
    "        **High-level goals:**\\n\\n\n",
    "        {highLevelGoals}\\n\n",
    "\n",
    "        **Output:**\n",
    "    \"\"\"\n",
    "\n",
    "    lowLevelGoals = generate_response(prompt, sys_prompt, LowLevelGoals)\n",
    "\n",
    "    return lowLevelGoals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation by Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation(description, actors, high_level_goals, low_level_goals):\n",
    "    sys_prompt = (\n",
    "        \"You're an helpful assistant, expert in the field of software engineering.\"\n",
    "        )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        You are provided with a software description, actors, high-level goals and low-level goals for said software.\\n\n",
    "        Actors, high-level goals and low-level goals were extracted by another assistant.\\n\n",
    "        Your job is to critique the work done by the assistant, scoring it on a scale from 0 to 10, assign a low score if you see any contradiction or important omissions.\\n\n",
    "        If the score is below 7, you should also provide a concise and specific feedback that will be used when retrying the task.\\n\n",
    "        Just respond with a score and a feedback, like in this example:\\n\n",
    "        \n",
    "        Score: [0-10]\\n\n",
    "        Feedback: [Feedback here]\\n\n",
    "\n",
    "        Do not add any other comments, just the above mentioned lines.\\n\n",
    "\n",
    "        **Description:** \\n\\n\n",
    "        {description}\n",
    "\n",
    "        **Actors:**\\n\\n\n",
    "        {actors}\n",
    "\n",
    "        **High-level goals:**\\n\\n\n",
    "        {high_level_goals}\n",
    "\n",
    "        **Low-level goals:**\\n\\n\n",
    "        {low_level_goals}\n",
    "\n",
    "        **Output:**\\n\\n\n",
    "    \"\"\"\n",
    "\n",
    "    critique = generate_response_llama(prompt, sys_prompt)\n",
    "    return critique \n",
    "\n",
    "def parse_evaluation(evaluation):\n",
    "    lines = evaluation.strip().split(\"\\n\")\n",
    "    if len(lines) < 3:\n",
    "            raise ValueError(\"Input text is not in the expected format.\")\n",
    "    score_line = lines[0]\n",
    "    if not score_line.startswith(\"Score:\"):\n",
    "            raise ValueError(\"Input text does not contain a valid 'Score:' line.\")\n",
    "    feedback_line = \" \".join(lines[2:])\n",
    "    if not feedback_line.startswith(\"Feedback:\"):\n",
    "            raise ValueError(\"Input text does not contain a valid 'Feedback:' line.\")\n",
    "    score = int(score_line.split(\":\")[1].strip())\n",
    "    feedback = feedback_line.split(\":\")[1].strip()\n",
    "    return score, feedback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get API List from Swagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class API(BaseModel):\n",
    "    api_name: str\n",
    "    api_path: str\n",
    "    description: str\n",
    "    request_type: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_list_from_swagger():\n",
    "    api_list = get_markdown(\"https://raw.githubusercontent.com/WebFuzzing/EMB/refs/heads/master/openapi-swagger/genome-nexus.json\")\n",
    "\n",
    "    json_api_list = json.loads(api_list)[\"paths\"]\n",
    "    api_paths = json_api_list.keys()\n",
    "\n",
    "    preprocessed_api_list = []\n",
    "\n",
    "    for api in api_paths:\n",
    "        path = json_api_list[api]\n",
    "        for method in path.keys():\n",
    "            preprocessed_api_list.append(\n",
    "                API(api_name=path[method][\"operationId\"], api_path=api, description=path[method][\"summary\"], request_type=method)\n",
    "            )\n",
    "            \n",
    "    return preprocessed_api_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping goal to API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIMapping(BaseModel):\n",
    "    APIs: list[API]\n",
    "    low_level_goal: LowLevelGoal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tabulate for nice table formatting\n",
    "from tabulate import tabulate\n",
    "\n",
    "def api_list_to_string(api_list):\n",
    "    apis = \"\"\n",
    "    for api in api_list:\n",
    "        apis += api.api_name + \", \"\n",
    "    # Remove the trailing comma and add a newline\n",
    "    apis = apis.rstrip(\", \") + \"\\n\"\n",
    "    return apis\n",
    "\n",
    "def define_mapping_apis_goals(lowLevelGoals, apiList, mode=Mode.ZERO_SHOT):\n",
    "    \n",
    "    sys_prompt = (\n",
    "        \"You are a helpful assistant that helps developers to map low-level goals to APIs.\"\n",
    "        \" You will be given a low-level goal and a list of APIs. Your task is to identify which APIs best satisfies each low-level goal.\"        \n",
    "        \"Respond with only the API name or 'No API Found' in the api_name field\"\n",
    "    )\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    for lowLevelgoal in lowLevelGoals.low_level_goals:\n",
    "        \n",
    "        #print(f\"Doing: {lowLevelgoal.get('description')} ..\" )\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "            Given the following goal:\n",
    "            {lowLevelgoal}\n",
    "\n",
    "            And the list of APIs below:\n",
    "            {apiList}\n",
    "\n",
    "            Identify the single API that best satisfies the goal. If no API satisfies the goal, return exactly \"No API Found\".\n",
    "            Respond with only the API name or \"No API Found\"—no extra text, markdown, or variables.\n",
    "\n",
    "            {(example1_map if mode == Mode.ONE_SHOT else f\"{example1_map}\\n\\n+{example2_map}\" if mode == Mode.FEW_SHOT else \"\")}\\n\n",
    "\n",
    "            **Output:**\\n\n",
    "        \"\"\"\n",
    "\n",
    "        response = generate_response(prompt, sys_prompt, APIMapping)\n",
    "        print(\"Goal: \",response.low_level_goal.description)\n",
    "        print(\"APIs: \", api_list_to_string(response.APIs))\n",
    "        result.append(response)\n",
    "\n",
    "        \n",
    "    return result\n",
    "\n",
    "        \n",
    "\n",
    "def print_api_goal_mapping(mappings):\n",
    "    \"\"\"\n",
    "    Prints the mapping between APIs and goals in a well-formatted table.\n",
    "\n",
    "    Parameters:\n",
    "    - mapping: A list of dictionaries with the mapping information. Each dictionary contains:\n",
    "        - 'low_level_goal': The goal.\n",
    "        - 'api': The API satisfying the goal or 'No API Found'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare data for tabulation\n",
    "        table_data = []\n",
    "        for mapping in mappings:\n",
    "            # Ensure entry contains expected keys and values\n",
    "            low_level_goal = mapping.low_level_goal.description\n",
    "            table_data.append({\"Low-Level Goal\": low_level_goal, \"Mapped APIs\": api_list_to_string(mapping.APIs)})\n",
    "        \n",
    "        # Print table with tabulate\n",
    "        print(tabulate(table_data, headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while printing mapping: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Description STARTING...\")\n",
    "description = get_description(\"https://raw.githubusercontent.com/WebFuzzing/EMB/refs/heads/master/openapi-swagger/genome-nexus.json\")\n",
    "print(\"Description DONE...\")\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actors STARTING...\")\n",
    "actors = define_actors(description)\n",
    "print(\"Actors DONE...\")\n",
    "print(actors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"High Level Goals STARTING...\")\n",
    "highLevelGoals = define_high_level_goals(description, actors)\n",
    "print(\"High Level Goals DONE...\")\n",
    "print(highLevelGoals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Low Level Goals STARTING...\")\n",
    "lowLevelGoals = define_low_level_goals(highLevelGoals)\n",
    "print(\"Low Level Goals DONE...\")\n",
    "print(lowLevelGoals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 3):\n",
    "    print(f\"Evaluation {i} by llama STARTING...\")\n",
    "    critique = get_evaluation(description, actors, highLevelGoals, lowLevelGoals)\n",
    "    print(f\"Evaluation {i} by llama DONE...\")\n",
    "    try:\n",
    "        # Parse the evaluation response\n",
    "        score, feedback = parse_evaluation(critique)\n",
    "        print(f\"Score: {score}\")\n",
    "        print(f\"Feedback: {feedback}\")\n",
    "        break\n",
    "    except ValueError as e:\n",
    "        print(f\"Error while parsing evaluation {i}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API List STARTING...\n",
      "API List DONE...\n",
      "[API(api_name='fetchVariantAnnotationPOST', api_path='/annotation', description='Retrieves VEP annotation for the provided list of variants', request_type='post'), API(api_name='fetchVariantAnnotationByIdPOST', api_path='/annotation/dbsnp/', description='Retrieves VEP annotation for the provided list of dbSNP ids', request_type='post'), API(api_name='fetchVariantAnnotationByIdGET', api_path='/annotation/dbsnp/{variantId}', description='Retrieves VEP annotation for the give dbSNP id', request_type='get'), API(api_name='fetchVariantAnnotationByGenomicLocationPOST', api_path='/annotation/genomic', description='Retrieves VEP annotation for the provided list of genomic locations', request_type='post'), API(api_name='fetchVariantAnnotationByGenomicLocationGET', api_path='/annotation/genomic/{genomicLocation}', description='Retrieves VEP annotation for the provided genomic location', request_type='get'), API(api_name='fetchVariantAnnotationGET', api_path='/annotation/{variant}', description='Retrieves VEP annotation for the provided variant', request_type='get'), API(api_name='fetchCanonicalEnsemblGeneIdByEntrezGeneIdsPOST', api_path='/ensembl/canonical-gene/entrez', description='Retrieves canonical Ensembl Gene ID by Entrez Gene Ids', request_type='post'), API(api_name='fetchCanonicalEnsemblGeneIdByEntrezGeneIdGET', api_path='/ensembl/canonical-gene/entrez/{entrezGeneId}', description='Retrieves Ensembl canonical gene id by Entrez Gene Id', request_type='get'), API(api_name='fetchCanonicalEnsemblGeneIdByHugoSymbolsPOST', api_path='/ensembl/canonical-gene/hgnc', description='Retrieves canonical Ensembl Gene ID by Hugo Symbols', request_type='post'), API(api_name='fetchCanonicalEnsemblGeneIdByHugoSymbolGET', api_path='/ensembl/canonical-gene/hgnc/{hugoSymbol}', description='Retrieves Ensembl canonical gene id by Hugo Symbol', request_type='get'), API(api_name='fetchCanonicalEnsemblTranscriptsByHugoSymbolsPOST', api_path='/ensembl/canonical-transcript/hgnc', description='Retrieves Ensembl canonical transcripts by Hugo Symbols', request_type='post'), API(api_name='fetchCanonicalEnsemblTranscriptByHugoSymbolGET', api_path='/ensembl/canonical-transcript/hgnc/{hugoSymbol}', description='Retrieves Ensembl canonical transcript by Hugo Symbol', request_type='get'), API(api_name='fetchEnsemblTranscriptsGET', api_path='/ensembl/transcript', description='Retrieves Ensembl Transcripts by protein ID, and gene ID. Retrieves all transcripts in case no query parameter provided', request_type='get'), API(api_name='fetchEnsemblTranscriptsByEnsemblFilterPOST', api_path='/ensembl/transcript', description='Retrieves Ensembl Transcripts by Ensembl transcript IDs, hugo Symbols, protein IDs, or gene IDs', request_type='post'), API(api_name='fetchEnsemblTranscriptByTranscriptIdGET', api_path='/ensembl/transcript/{transcriptId}', description='Retrieves the transcript by an Ensembl transcript ID', request_type='get'), API(api_name='fetchGeneXrefsGET', api_path='/ensembl/xrefs', description='Perform lookups of Ensembl identifiers and retrieve their external references in other databases', request_type='get'), API(api_name='fetchPdbHeaderPOST', api_path='/pdb/header', description='Retrieves PDB header info by a PDB id', request_type='post'), API(api_name='fetchPdbHeaderGET', api_path='/pdb/header/{pdbId}', description='Retrieves PDB header info by a PDB id', request_type='get'), API(api_name='fetchPfamDomainsByPfamAccessionPOST', api_path='/pfam/domain', description='Retrieves PFAM domains by PFAM domain accession IDs', request_type='post'), API(api_name='fetchPfamDomainsByAccessionGET', api_path='/pfam/domain/{pfamAccession}', description='Retrieves a PFAM domain by a PFAM domain ID', request_type='get'), API(api_name='fetchPostTranslationalModificationsGET', api_path='/ptm/experimental', description='Retrieves PTM entries by Ensembl Transcript ID', request_type='get'), API(api_name='fetchPostTranslationalModificationsByPtmFilterPOST', api_path='/ptm/experimental', description='Retrieves PTM entries by Ensembl Transcript IDs', request_type='post'), API(api_name='fetchVersionGET', api_path='/version', description='Retrieve Genome Nexus Version', request_type='get')]\n"
     ]
    }
   ],
   "source": [
    "print(\"API List STARTING...\")\n",
    "apiList = get_api_list_from_swagger()\n",
    "print(\"API List DONE...\")\n",
    "print(apiList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mapping STARTING...\")\n",
    "mappings = define_mapping_apis_goals(lowLevelGoals, apiList)\n",
    "print(\"Mapping DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prettier\n",
    "print(\"\\n\\n\")\n",
    "print_api_goal_mapping(mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Description STARTING...\")\n",
    "description = get_description(\"https://raw.githubusercontent.com/WebFuzzing/EMB/refs/heads/master/openapi-swagger/genome-nexus.json\")\n",
    "print(\"Description DONE...\")\n",
    "print(description)\n",
    "\n",
    "print(\"Actors STARTING...\")\n",
    "actors = define_actors(description)\n",
    "print(\"Actors DONE...\")\n",
    "print(actors)\n",
    "\n",
    "# whole loop for the application\n",
    "MAX_ATTEMPTS = 5\n",
    "attempts_count = 0\n",
    "feedback = None\n",
    "while True < MAX_ATTEMPTS:\n",
    "    attempts_count += 1\n",
    "    print(\"High Level Goals STARTING...\")\n",
    "    highLevelGoals = define_high_level_goals(description, actors, feedback=feedback)\n",
    "    print(\"High Level Goals DONE...\")\n",
    "    print(highLevelGoals)\n",
    "\n",
    "    print(\"Low Level Goals STARTING...\")\n",
    "    lowLevelGoals = define_low_level_goals(highLevelGoals, feedback=feedback)\n",
    "    print(\"Low Level Goals DONE...\")\n",
    "    print(lowLevelGoals)\n",
    "\n",
    "    # do ten attempts at evaluation, these may fail if parsing fails\n",
    "    for _ in range(1, 10):\n",
    "        print(f\"Evaluation by llama STARTING...\")\n",
    "        eval = get_evaluation(description, actors, highLevelGoals, lowLevelGoals)\n",
    "        print(f\"Evaluation by llama DONE...\")\n",
    "        try:\n",
    "            # Parse the evaluation response\n",
    "            score, critique = parse_evaluation(eval)\n",
    "            print(f\"Score: {score}\")\n",
    "            print(f\"Critique: {critique}\")\n",
    "\n",
    "            # log this to check output\n",
    "            with open(\"output.txt\", \"a\") as file:  # Use \"w\" to overwrite or \"a\" to append\n",
    "                file.write(f\"Critique: {critique}\\nScore: {score}\\nHLG: {highLevelGoals}\\nLLG: {lowLevelGoals}\\n\\n\\n\")\n",
    "\n",
    "            if score >= 8:\n",
    "                print(\"Satisfactory score achieved! Breaking out of the loop.\")\n",
    "                break\n",
    "        except ValueError as e:\n",
    "                print(f\"Error while parsing evaluation {i}: {e}\")\n",
    "    else:\n",
    "            # If both evaluations fail to achieve a satisfactory score\n",
    "            print(\"No satisfactory score achieved. Retrying...\")\n",
    "            feedback = Feedback(f\"{actors}\\n{highLevelGoals}\\n{lowLevelGoals}\", critique=critique)\n",
    "            continue\n",
    "        # Exit the outer loop if a satisfactory score was achieved\n",
    "    if score >= 8:\n",
    "        break\n",
    "\n",
    "if attempts_count== MAX_ATTEMPTS and score < 8:\n",
    "    print(f\"Max attempts ({MAX_ATTEMPTS}) reached. Could not achieve a satisfactory score.\")\n",
    "\n",
    "print(\"API List STARTING...\")\n",
    "apiList = get_api_list_from_swagger()\n",
    "print(\"API List DONE...\")\n",
    "print(apiList)\n",
    "\n",
    "print(\"Mapping STARTING...\")\n",
    "mappings = define_mapping_apis_goals(lowLevelGoals, apiList)\n",
    "print(\"Mapping DONE\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print_api_goal_mapping(mappings)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0adcbace0f6946568311b8010d5c7fc3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14d5cc69e04e4fe588144913d220e57f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b52835f91344aa396de748f5a68e40e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c93510709bd46c889be8a8872af280e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d7aec60591343bab6a920285a64112f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ffc14edebc2432f9b95e774167ed333": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6422c15135a24be783734f255088421f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "99beab2c378044f9833caf3d6b6f3d4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0adcbace0f6946568311b8010d5c7fc3",
      "placeholder": "​",
      "style": "IPY_MODEL_2b52835f91344aa396de748f5a68e40e",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "d3c2cd7f501c4c93ab43730f061273b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d7aec60591343bab6a920285a64112f",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6422c15135a24be783734f255088421f",
      "value": 2
     }
    },
    "f795283dfaad4da2bb0b3fce64f7f8c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c93510709bd46c889be8a8872af280e",
      "placeholder": "​",
      "style": "IPY_MODEL_14d5cc69e04e4fe588144913d220e57f",
      "value": " 2/2 [00:02&lt;00:00,  1.38s/it]"
     }
    },
    "fe25ee512c184077be919dcd38d43296": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_99beab2c378044f9833caf3d6b6f3d4e",
       "IPY_MODEL_d3c2cd7f501c4c93ab43730f061273b5",
       "IPY_MODEL_f795283dfaad4da2bb0b3fce64f7f8c2"
      ],
      "layout": "IPY_MODEL_5ffc14edebc2432f9b95e774167ed333"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
